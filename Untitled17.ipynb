{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7h4dBWhi5Gt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JKWYI3CXi5ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                  SVM & Naive bayes\n",
        "#Theoretical\n",
        "1.: What is a Support Vector Machine (SVM) ?\n",
        "- A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. It is especially effective in high-dimensional spaces and for problems where the number of dimensions exceeds the number of samples.\n",
        "2. What is the difference between Hard Margin and Soft Margin SVM ?\n",
        "- The difference between Hard Margin and Soft Margin SVM lies in how strictly the algorithm separates the classes and handles misclassifications:\n",
        "3. : What is the mathematical intuition behind SVM ?\n",
        "- The mathematical intuition behind Support Vector Machines (SVM) revolves around finding the optimal separating hyperplane that maximizes the margin between two classes\n",
        "1. Goal of SVM 2. Equation of a Hyperplane\n",
        "4. What is the role of Lagrange Multipliers in SVM ?\n",
        "- The role of Lagrange multipliers in Support Vector Machines (SVM) is central to transforming the constrained optimization problem (primal form) into a dual problem, which is easier to solve‚Äîespecially when using kernel functions for non-linear data\n",
        "5. What are Support Vectors in SVM\n",
        "- Support Vectors are the key data points in a Support Vector Machine (SVM) model that:\n",
        "\n",
        "Lie closest to the decision boundary (hyperplane).\n",
        "\n",
        "Define the position and orientation of the optimal hyperplane.\n",
        "\n",
        "Are the \"most difficult\" examples to classify correctly.\n",
        "\n",
        "Directly influence the margin width ‚Äî the distance between classes\n",
        "6. What is a Support Vector Classifier (SVC) ?\n",
        "- A Support Vector Classifier:\n",
        "\n",
        "Is a supervised learning model that uses the SVM algorithm to classify data points into categories (e.g., positive vs. negative).\n",
        "\n",
        "Finds the optimal hyperplane (or decision boundary) that maximizes the margin between different classes.\n",
        "7. What is a Support Vector Regressor (SVR) ?\n",
        "- A Support Vector Regressor (SVR) is the regression version of Support Vector Machines (SVM). Instead of classifying data into categories like SVC, SVR predicts continuous numerical values.\n",
        "\n",
        "8. What is the Kernel Trick in SVM ?\n",
        "- The Kernel Trick is a fundamental technique in Support Vector Machines (SVMs) that allows them to perform non-linear classification or regression by implicitly mapping data into a higher-dimensional space ‚Äî without explicitly computing that transformation.\n",
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel ?\n",
        "-\n",
        "10. What is the effect of the C parameter in SVM ?\n",
        "- The C parameter in SVM (Support Vector Machine) is a regularization parameter that controls the trade-off between:\n",
        "\n",
        "Maximizing the margin (keeping the model simple)\n",
        "\n",
        "Minimizing classification error on the training data\n",
        "\n",
        "11. What is the role of the Gamma parameter in RBF Kernel SVM ?\n",
        "- The Gamma (Œ≥) parameter in an RBF (Radial Basis Function) kernel SVM controls the influence radius of a single training example, shaping the complexity and flexibility of the decision boundary.\n",
        "12. : What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve ?\n",
        "- The Na√Øve Bayes classifier is a simple and powerful probabilistic machine learning algorithm used for classification tasks. It is based on applying Bayes' theorem with a strong assumption of feature independence.\n",
        "13. : What is Bayes‚Äô Theorem ?\n",
        "- Bayes‚Äô Theorem is a fundamental rule in probability theory that describes how to update the probability of a hypothesis based on new evidence.\n",
        "14. Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes ?\n",
        "- Great question! Here‚Äôs a clear explanation of the three main types of Na√Øve Bayes classifiers, distinguished by the kind of data they model and the assumptions they make about feature distributions:\n",
        "\n",
        "Assumption: Features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Use case: Continuous-valued features (e.g., height, weight, temperature).\n",
        "15. When should you use Gaussian Na√Øve Bayes over other variants ?\n",
        "- Data type: Your input features are continuous numerical variables (e.g., height, weight, temperature, blood pressure).\n",
        "\n",
        "Distribution: The features for each class approximately follow a bell-shaped (Gaussian) distribution.\n",
        "\n",
        "Use case examples:\n",
        "\n",
        "Medical diagnosis using lab measurements.\n",
        "\n",
        "Sensor data classification.\n",
        "\n",
        "Any scenario with real-valued measurements.\n",
        "16.  What are the key assumptions made by Na√Øve Bayes ?\n",
        "- 1. Conditional Independence of Features\n",
        "This means each feature\n",
        "ùë•\n",
        "ùëñ\n",
        "x\n",
        "i\n",
        "‚Äã\n",
        "  is assumed independent of every other feature\n",
        "ùë•\n",
        "ùëó\n",
        "x\n",
        "j\n",
        "‚Äã\n",
        "  given the class\n",
        "ùê∂\n",
        "C.\n",
        "\n",
        "In reality, features are often correlated, so this assumption is called ‚Äúna√Øve‚Äù because it‚Äôs usually not true.\n",
        "\n",
        "This assumption greatly simplifies computation of the joint likelihood\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "P(x‚à£C) as a product of individual likelihoods:\n",
        "17.  What are the advantages and disadvantages of Na√Øve Bayes ?\n",
        "Advantages\n",
        "Simple and Fast\n",
        "\n",
        "Easy to implement and computationally efficient.\n",
        "\n",
        "Works well even with small training data.\n",
        "\n",
        "Handles High-Dimensional Data\n",
        "\n",
        "Performs well with many features (e.g., text classification with thousands of words).\n",
        "\n",
        "Works Well with Both Discrete and Continuous Data\n",
        "\n",
        "Variants exist for different data types (Gaussian, Multinomial, Bernoulli).\n",
        " Disadvantages\n",
        "Strong Independence Assumption\n",
        "\n",
        "Assumes features are conditionally independent given the class, which is often not true in real data.\n",
        "\n",
        "Violations can degrade accuracy.\n",
        "\n",
        "Poor Estimation of Probabilities\n",
        "\n",
        "The probabilities produced may not be well-calibrated if the independence assumption fails.\n",
        "\n",
        "Zero Frequency Problem\n",
        "\n",
        "If a categorical feature value never appears in training data for a class, it results in zero probability (can be handled by smoothing techniques like Laplace smoothing).\n",
        "18. : Why is Na√Øve Bayes a good choice for text classification ?\n",
        "- 1. Handles High-Dimensional Data Efficiently\n",
        "Text data often has thousands or millions of features (words or n-grams).\n",
        "\n",
        "Na√Øve Bayes scales well and remains computationally efficient even with very large vocabularies.\n",
        "\n",
        "2. Works Well with Sparse Data\n",
        "Most documents contain only a small subset of all possible words (sparse feature vectors).\n",
        "\n",
        "Na√Øve Bayes naturally handles sparse input without requiring complex preprocessing.\n",
        "\n",
        "\n",
        "19. Compare SVM and Na√Øve Bayes for classification tasks ?\n",
        "-\n",
        "20. How does Laplace Smoothing help in Na√Øve Bayes ?\n",
        "- When a categorical feature value never appears in the training data for a given class, the likelihood\n",
        "ùëÉ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "P(x\n",
        "i\n",
        "‚Äã\n",
        " ‚à£C) becomes zero.\n",
        "\n",
        "Since Na√Øve Bayes multiplies all feature probabilities, a single zero probability makes the entire product zero, effectively ruling out that class.\n",
        "\n",
        "This can cause poor generalization on unseen data.\n",
        "\n",
        "#Practical\n",
        "1. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy ?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S3ZnVG27i7QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM classifier on Iris test set: {a\n"
      ],
      "metadata": {
        "id": "sMXvWUa0m78l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. : Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies ?\n"
      ],
      "metadata": {
        "id": "NIRJCyQ0nA_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize two SVM classifiers: Linear and RBF kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train both classifiers\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accura\n"
      ],
      "metadata": {
        "id": "k9vjw4nwnE7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE): ?\n"
      ],
      "metadata": {
        "id": "3vOpo1sYnJTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVR model with RBF kernel (default)\n",
        "svr_model = SVR(kernel='rbf')\n",
        "\n",
        "# Train the SVR\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "zAh_-jAMnPej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "boundary ?\n"
      ],
      "metadata": {
        "id": "srvKwq90nTaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset and select two features for 2D visualization\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # use first two features\n",
        "y = iris.target\n",
        "\n",
        "# For simplicity, use only two classes (binary classification)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Standardize features for better performance\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train SVM with polynomial kernel (degree 3)\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, coef0=1, gamma='scale')\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Create a mesh grid for plotting decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X\n"
      ],
      "metadata": {
        "id": "x-Bed1rWoIGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. : Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy ?"
      ],
      "metadata": {
        "id": "Vf1RKRlToTKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naive Bayes on Breast Cancer dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Sbh5eboHoZ4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset"
      ],
      "metadata": {
        "id": "q_d_y4t1ofCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load 20 Newsgroups dataset (subset for faster training)\n",
        "newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split data into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline that:\n",
        "# 1. Converts text to token counts\n",
        "# 2. Transforms counts to TF-IDF features\n",
        "# 3. Trains a Multinomial Naive Bayes classifier\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),         # Tokenize and count word occurrences\n",
        "    ('tfidf', TfidfTransformer()),       # Transform counts to TF-IDF scores\n",
        "    ('clf', MultinomialNB()),            # Multinomial Naive Bayes classifier\n",
        "])\n",
        "\n",
        "# Train the classifier\n",
        "text_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = text_clf.predict(X_test_\n"
      ],
      "metadata": {
        "id": "s8om1iWgoj-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "boundaries visually ?\n"
      ],
      "metadata": {
        "id": "4WejhuIsopKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CqNJrbK1ot9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset and use only two classes & two features for 2D visualization\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# Use only classes 0 and 1 for binary classification\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define different C values to compare\n",
        "C_values = [0.01, 1, 100]\n",
        "\n",
        "# Create m\n"
      ],
      "metadata": {
        "id": "D1xESPqfouYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with\n",
        "binary features"
      ],
      "metadata": {
        "id": "0UcRllsdoyig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a simple synthetic dataset with binary features\n",
        "# Features are 0 or 1, and target is binary class 0 or 1\n",
        "X = np.array([\n"
      ],
      "metadata": {
        "id": "cJ9gO4gBo2xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "unscaled data"
      ],
      "metadata": {
        "id": "RNzXvpxRo6pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM on unscaled data\n",
        "svm_unscaled = SVC(kernel='rbf', random_state=42)\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scale\n"
      ],
      "metadata": {
        "id": "tGs6xcHYo_WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and\n",
        "after Laplace Smoothing ?"
      ],
      "metadata": {
        "id": "xTDWksDmpFSd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPhcJeBLpG-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. \u0017= Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "gamma, kernel)"
      ],
      "metadata": {
        "id": "mSZe5EFhpL1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1,]()\n"
      ],
      "metadata": {
        "id": "yJva5wLxpQHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy="
      ],
      "metadata": {
        "id": "65NMAKW8pT8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create an imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=2, n_redundant=10,\n",
        "                           n_clusters_per_class=1,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM\n"
      ],
      "metadata": {
        "id": "3yKCOYZbpZe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data"
      ],
      "metadata": {
        "id": "jp1cwnr6pdPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Define spam and ham-like categories from 20 Newsgroups for demo\n",
        "# 'rec.sport.hockey' (ham) and 'talk.politics.misc' (spam-like)\n",
        "categories = ['rec.sport.hockey', 'talk.politics.misc']\n",
        "\n",
        "# Load dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X = ne\n"
      ],
      "metadata": {
        "id": "2TmS7SAtphc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and\n",
        "compare their accuracy"
      ],
      "metadata": {
        "id": "s8Yw0RkXplcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_clf = SVC(kernel='rbf', random_state=42)\n",
        "svm_clf.fit(X_\n"
      ],
      "metadata": {
        "id": "-56P-4pSppsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare\n",
        "results="
      ],
      "metadata": {
        "id": "lJw4L6ZxptH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Scale features to [0,1] because chi2 requires non-negative values\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train\n"
      ],
      "metadata": {
        "id": "0W4QglKbpxk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. = Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "strategies on the Wine dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "-E0qxb-4p1_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_tes\n"
      ],
      "metadata": {
        "id": "mUYmQiRIp57N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "Cancer dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "bEzNt5_Cp99c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define kernels to compare\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "# Dictionary to sto\n"
      ],
      "metadata": {
        "id": "Via9izDzqBvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "average accuracy=\n"
      ],
      "metadata": {
        "id": "kWHV5PoNqFYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_clf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Initialize Stratified K-Fold cross-validator with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and compute accuracy for each fold\n",
        "cv_scores = cross_val_score(svm_clf, X, y, cv=skf, sco\n"
      ],
      "metadata": {
        "id": "GqDHFsHoqJR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare\n",
        "performance\n"
      ],
      "metadata": {
        "id": "g5m0Vd4_qPGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different prior probabilities\n",
        "priors_list = [\n",
        "    None,                     # Use class frequencies from training data\n",
        "    [0.5, 0.25, 0.25],       # Custom prior probabilities for 3 classes\n",
        "    [0.33, 0.33, 0.34]       # Another set of priors\n",
        "]\n",
        "\n",
        "for i, priors in enumerate(priors_list):\n",
        "    # Initialize Gaussian Naive Bayes with priors\n",
        "    gnb = GaussianNB(priors=priors)\n",
        "    gnb.fit(X_train, y_train)\n",
        "    y_pred = gnb.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    prior_text = \"default (from data)\" if priors is None else str(priors)\n",
        "    print(f\"Accuracy with priors {prior_text}: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "ojN3-sdRqRJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "compare accuracy"
      ],
      "metadata": {
        "id": "e-wg6xlMqVq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM without feature selection\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "accuracy_without_rfe = accura\n"
      ],
      "metadata": {
        "id": "IpoUXZ0qqZR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}